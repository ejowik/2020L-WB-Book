## Can Automated Regression beat linear model?

*Authors: Bartłomiej Granat, Szymon Maksymiuk, (Warsaw University of Technology)*

### Abstract


### Introduction and Motivation

Health-related problems have been a topic of multiple papers throughout the years and machine learning brought some new methods to modern medicine.[Potrzebna cytacja jakiegoś medycznego papieru]. We care so much about our lives that every single algorithm and method eventually gets tested on some medical data.

What is unique about health data is that black-box models are completely useless in this subject. Almost always doctors know whether a patient is sick or not. What is important to them is the reason why he is sick. That's why explainable machine learning is the key to make all of us healthier. However XAI methods are not very well defined yet and they lack strong statistical background like tests, etc. That caused strong opposition of Explainable Machine Learning to rise[@Rudin2019]. Unfortunately making a good explainable model for health data might be close to impossible. Medical problems of all kinds can be very unique, complex, or completely random. That's why researchers spend numerous hours on improving their explainable models and that's why we decided to test our approach on `liver disorders` dataset with help. We will try to improve linear model results with the help of AutoML black-box model.

There is a big discussion in the scientific world about "what does it mean for a model to be interpretable?"[@Lipton]. Unfortunately there is no clear answer to that question and often it is necessary to clearly specify what is supposed to be meant as interpretable. In this paper we assume that the model is interpretable if it is possible to explicitly track down what contributed to model answer and to do it in a finite time. It means that all transformation of variables or even concatenations are possible ass soon as a used type of model is simply structured like a linear model is. However, we understand other's opinions and we will present also results with only simple transformations and without controversial concatenations.

The `liver-disorders` dataset is well known in the field of machine learning[cytacja do tego artykułu o tym ddataset] and that's exactly the reason why we chose it. It is described in the next chapter. Our goal was to find a relatively clean dataset with many models already done by other researchers. Another advantage is the availability of the dataset. It is published on the OpenML repository and therefore everyone can give a shot to that problem. We don't want to show that properly cleaned data gives better results but to achieve, an explainable model found after a complex analysis that we want to test.

In this paper we do a case study on `liver disorders` dataset and want to prove that by using automated regression it is possible to build an easy to understand prediction that outperforms black-box models on the real dataset and at the same time achieve similar results to other researchers. By automated regression we understand a process when we search through space of aviable dataset transformation and try to find the one for which linear regression model is best using earlier defined loss function. 

### Data

The dataset we use to test our hypothesis is a well-known `liver-disorders` first created by 'BUPA Medical Research Ltd.' containing a single male patient as a row. The data consists of 5 features which are the results of blood tests a physician might use to inform diagnosis. There is no ground truth in the data set relating to the presence or absence of a disorder. The target feature is attribute drinks, which are numerical. Some of the researchers tend to split the patients into 2 groups: 0 - patients that drink less than 3 half-pint equivalents of alcoholic beverages per day and 1 - patients that drink more or equal to 3 and focus on a classification problem.

All of the features are numerical. The data is available for 345 patients and contains 0 missing values. 

The dataset consists of 7 attributes:

1. mcv - mean corpuscular volume
2. alkphos - alkaline phosphatase
3. sgpt - alanine aminotransferase
4. sgot - aspartate aminotransferase
5. gammagt - gamma-glutamyl transpeptidase 
6. drinks - number of half-pint equivalents of alcoholic beverages drunk per day
7. selector - field created by the BUPA researchers to split the data into train/test sets

For further readings on the dataset and misunderstandings related to the selector column incorrectly treated as target refer to: "McDermott & Forsyth 2016, Diagnosing a disorder in a classification benchmark, Pattern Recognition Letters, Volume 73."


### Methodology

AutoMl Model $M_{aml}$ and the dataset $D$ that consists of $D_{X} = X$ which is set of independent variables and $D_{y} = y$ - dependent variable (ie. target). We assume that $M_{aml}$ is an unknown function $M_{aml}: \mathbb{R}^{p} \to \mathbb{R}$, where p is a snumber of features in the $D$ Dataset, that satisfies $y = M_{aml}(X) + \epsilon$ where $\epsilon$ is an error vector. Automated regression constructs known vector function $$G_{AR} : \mathbb{R}^{n \times p} \to \mathbb{R}^{n \times p_{1}}$$ where $n$ is a number of observations, that satisfies $$y = G_{AR}(X)\beta + \epsilon$$ thus it is linear regression model fitted for transformated data. $p_{1}$ does not have to equal $p$ since we allow conncatenations of variables.

To find $G_{AR}$ we have to put some constraints. First of all we want it to minimize loss function one of two loss functions $L: \mathbb{R}^{n} \to \mathbb{R}$. First of them is $$L_{R} : \frac{\sqrt{\sum_{i=1}^{n}(y_{i}-\hat{y_{i}})^{2}}\sum_{i=1}^{n}(y_{i}-\bar{y_{i}})^{2}}{\sum_{i=1}^{n}(\hat{y_{i}}-\bar{y_{i}})^{2}}$$ which can be interpreted as Root Mean Square Error divided by the R-squred coefficient of determination. R-squared stands as measure of variance explained by the model [@3-7-houseprices-tree] and therfore may be usefull to find best explanation of interactions met in the data. Obviously high coefficient does not always mean excelent model, furthermore even low values of it not alwyays inform us about uselessness of found fit. Therefore minimization will be prefromed independetly using second mesure as well $$L_{0} : \sqrt{\sum_{i=1}^{n}(y_{i}-\hat{y_{i}})^{2}}$$ which is Root of the Mean Squared Error (RMSE). One top of that we also put constrains on domain of valid transformations of particular variables. For given dataset, described in the previous paragraphs we decided to use:

* Feature selection
  + XAI feature Importance
* Continuous transformation
  + Polynomial transformation 
  + Lograthmic transformation
* Discrete transformation
  + SAFE method
* Feature concatenation
  + Multiplication of pair of features.


Obviously, XAI related methods are conducted using AutoML baseline Model. We've decided to omit data imputation as an element of valid transformations set because liver-disorders dataset does not meet with the problem of missing values. 

The optimization process is conducted based on Bayesian Optimization and the backtracing idea. Each main element of the domain of valid transformations is one step in the process of creation $G_{AR}$ function. Within each step, Bayesian optimization will be used to find the best transformation for the given level. During further steps, if any of transformation did not improve model, ie. $L$ function was only growing, the algorithm takes second, the third, etc. solution from previous steps according to backtracking idea. If for no one of $k$ such iterations, where k is known parameter, a better solution is found, step is omitted.

### Results


### Summary and conclusions 

